{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3gR36IigTDG+5Zw11eW8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaUnal/probability/blob/main/AutoSuggest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA7Nvpl3AiCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c63c2de-8cf4-49f8-8201-d0bfc0775414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from urllib import request\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading all the given phrases and storing in a list input_corpus"
      ],
      "metadata": {
        "id": "UPbJcS6kfKJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = [\"http://www.gutenberg.org/files/2554/2554-0.txt\",\"https://www.gutenberg.org/files/730/730-0.txt\"]\n",
        "input_corpus=[]\n",
        "for i in range(len(url)):\n",
        "    response = request.urlopen(url[i])\n",
        "    raw = response.read().decode('utf-8-sig')\n",
        "    temp_words = word_tokenize(raw)\n",
        "    input_corpus.extend(temp_words)\n",
        "    temp_words = []"
      ],
      "metadata": {
        "id": "fjLcsINRA125"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this function checks the occurence in first three words;if not found then next 2;else finds the last word\n"
      ],
      "metadata": {
        "id": "sk_ADtZufGif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggestion(inpu,words):\n",
        "\n",
        "    three=[]\n",
        "    two=[]\n",
        "    one=[]\n",
        "\n",
        "    if len(inpu)>=3:\n",
        "        for i in range(len(words)-3):\n",
        "            if words[i]==inpu[len(inpu)-3] and words[i+1]==inpu[len(inpu)-2] and words[i+2]==inpu[len(inpu)-1]:\n",
        "               three.append(i+3)\n",
        "        if len(three) !=0:\n",
        "            return three\n",
        "\n",
        "    if (len(three)==0 or len(inpu)<3) and len(inpu)>1:\n",
        "        for i in range(len(words)-2):\n",
        "            if words[i]==inpu[len(inpu)-2] and words[i+1]==inpu[len(inpu)-1]:\n",
        "                two.append(i+2)\n",
        "        if len(two) !=0:\n",
        "            return two\n",
        "\n",
        "    if len(three)==0 and len(two)==0:\n",
        "        for i in range(len(words)-1):\n",
        "            if words[i]==inpu[len(inpu)-1]:\n",
        "                one.append(i+1)\n",
        "        return one\n"
      ],
      "metadata": {
        "id": "BUImBJzuA4Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_prob(inpu,words):\n",
        "    arr_ = suggestion(inpu,input_corpus)\n",
        "    if len(arr_)==0:     #returns nothing if the word in input is not present in the corpus\n",
        "      return '','Undefined'\n",
        "    arr=[]\n",
        "    for x in arr_:\n",
        "        arr.append(words[x])\n",
        "    a = Counter(arr)\n",
        "    prob = round(a[max(a,key= lambda x:a[x])]/sum(a.values()),3)\n",
        "    X = arr.index(max(a,key= lambda x:a[x]))\n",
        "    ans = max(a,key= lambda x:a[x]) + \" \" + words[arr_[X]+1]\n",
        "    return ans,prob"
      ],
      "metadata": {
        "id": "X4wuFOoZdUjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stores all the input for which suggestion has to be given\n"
      ],
      "metadata": {
        "id": "Zgxrx2UVe94j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phr= []\n",
        "filename = \"AutoComplete.txt\"\n",
        "with open(filename) as f:\n",
        "    for line in f:\n",
        "     phr.append(line.rstrip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Pmq6PIkleVdB",
        "outputId": "96e7c5a7-668f-4fd4-c68a-81587d9cef7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-da495ab9d38a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstr_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"AutoComplete.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mstr_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AutoComplete.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output th suggestion along with the probability"
      ],
      "metadata": {
        "id": "SMn0MBrBfg0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(phr)):\n",
        "    inpu=[]\n",
        "    for x in phr[i].split():\n",
        "        if x.isalpha():\n",
        "            inpu.append(x.lower())\n",
        "    print(phr [i]+ \" \"+ max_prob(inpu,input_corpus)[0])\n",
        "    print(\"Probability:\",max_prob(inpu,input_corpus)[1])"
      ],
      "metadata": {
        "id": "s0hgK-HVebSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ELWlAy9jBP1F"
      }
    }
  ]
}